"""
ChangeLens API ä¸»æœåŠ¡
ChangeLens API Main Service

æä¾›RESTful APIç«¯ç‚¹ï¼Œæ”¯æŒæ•°æ®æŸ¥è¯¢ã€ä»»åŠ¡å¤„ç†ã€å›å½’æ³¨å…¥å’ŒæŒ‡æ ‡ç›‘æ§
"""
import time
import asyncio
from datetime import datetime
from typing import List, Optional
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Depends
from fastapi.responses import JSONResponse
import asyncpg
import httpx
from redis import Redis

from app.config import settings
from app.models import Item, ProcessRequest, ProcessResponse, MetricsResponse, RegressionRequest
from app.regression_injector import regression_injector

# å…¨å±€å˜é‡ç”¨äºå­˜å‚¨æŒ‡æ ‡
# Global variables for storing metrics
metrics_store = {
    "latencies": [],  # å­˜å‚¨æ‰€æœ‰è¯·æ±‚å»¶è¿Ÿ / Store all request latencies
    "errors": 0,  # é”™è¯¯è®¡æ•° / Error count
    "requests": 0,  # è¯·æ±‚è®¡æ•° / Request count
    "start_time": time.time()
}

# æ•°æ®åº“è¿æ¥æ±  / Database connection pool
db_pool: Optional[asyncpg.Pool] = None
redis_client: Optional[Redis] = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç† / Application lifecycle management"""
    global db_pool, redis_client
    
    # å¯åŠ¨æ—¶åˆå§‹åŒ–è¿æ¥ / Initialize connections on startup
    try:
        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± 
        # Initialize database connection pool
        db_pool = await asyncpg.create_pool(
            settings.database_url.replace("postgresql+asyncpg://", "postgresql://"),
            min_size=5,
            max_size=20
        )
        
        # åˆå§‹åŒ–Redisè¿æ¥
        # Initialize Redis connection
        redis_client = Redis.from_url(settings.redis_url, decode_responses=True)
        
        print("âœ… æ•°æ®åº“å’ŒRedisè¿æ¥å·²å»ºç«‹ / Database and Redis connections established")
    except Exception as e:
        print(f"âŒ è¿æ¥åˆå§‹åŒ–å¤±è´¥ / Connection initialization failed: {e}")
    
    yield
    
    # å…³é—­æ—¶æ¸…ç†è¿æ¥ / Cleanup connections on shutdown
    if db_pool:
        await db_pool.close()
    if redis_client:
        redis_client.close()
    print("ğŸ”Œ è¿æ¥å·²å…³é—­ / Connections closed")


app = FastAPI(
    title="ChangeLens API",
    description="Change-Induced Performance Regression & Safe Release Benchmark API",
    version="1.0.0",
    lifespan=lifespan
)


async def get_db():
    """è·å–æ•°æ®åº“è¿æ¥ / Get database connection"""
    if db_pool is None:
        raise HTTPException(status_code=503, detail="æ•°æ®åº“è¿æ¥ä¸å¯ç”¨ / Database connection unavailable")
    return db_pool


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹ / Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "changelens-api"
    }


@app.get("/api/data", response_model=List[Item])
async def get_data(db=Depends(get_db)):
    """
    è·å–æ•°æ®ç«¯ç‚¹ï¼ˆä¼šè§¦å‘æ•°æ®åº“æŸ¥è¯¢ï¼‰
    Get data endpoint (triggers database query)
    """
    start_time = time.time()
    
    try:
        # æ³¨å…¥æ•°æ®åº“å›å½’ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        # Inject DB regression (if enabled)
        await regression_injector.inject_db_regression()
        
        # æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢
        # Execute database query
        async with db.acquire() as conn:
            rows = await conn.fetch(
                "SELECT id, name, value, created_at, updated_at FROM items ORDER BY created_at DESC LIMIT 100"
            )
        
        items = [
            Item(
                id=row["id"],
                name=row["name"],
                value=row["value"],
                created_at=row["created_at"],
                updated_at=row["updated_at"]
            )
            for row in rows
        ]
        
        # è®°å½•å»¶è¿Ÿ / Record latency
        latency_ms = (time.time() - start_time) * 1000
        metrics_store["latencies"].append(latency_ms)
        metrics_store["requests"] += 1
        
        return items
    
    except Exception as e:
        metrics_store["errors"] += 1
        metrics_store["requests"] += 1
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢å¤±è´¥ / Query failed: {str(e)}")


@app.post("/api/process", response_model=ProcessResponse)
async def process_task(request: ProcessRequest):
    """
    å¤„ç†ä»»åŠ¡ç«¯ç‚¹ï¼ˆæäº¤å¼‚æ­¥ä»»åŠ¡åˆ°Workerï¼‰
    Process task endpoint (submit async task to Worker)
    """
    start_time = time.time()
    
    try:
        # æ³¨å…¥CPUå›å½’ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        # Inject CPU regression (if enabled)
        await regression_injector.inject_cpu_regression()
        
        # æ³¨å…¥ä¾èµ–å›å½’ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        # Inject dependency regression (if enabled)
        try:
            await regression_injector.inject_dependency_regression()
        except TimeoutError:
            # ä¾èµ–è¶…æ—¶ï¼Œè®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†
            # Dependency timeout, record error but continue processing
            metrics_store["errors"] += 1
        
        # æäº¤ä»»åŠ¡åˆ°WorkeræœåŠ¡
        # Submit task to Worker service
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{settings.worker_url}/api/tasks",
                json={"task_id": request.task_id, "data": request.data},
                timeout=5.0
            )
            response.raise_for_status()
        
        # è®°å½•å»¶è¿Ÿ / Record latency
        latency_ms = (time.time() - start_time) * 1000
        metrics_store["latencies"].append(latency_ms)
        metrics_store["requests"] += 1
        
        return ProcessResponse(
            task_id=request.task_id,
            status="submitted",
            message="ä»»åŠ¡å·²æäº¤ / Task submitted"
        )
    
    except httpx.TimeoutException:
        metrics_store["errors"] += 1
        metrics_store["requests"] += 1
        raise HTTPException(status_code=504, detail="WorkeræœåŠ¡è¶…æ—¶ / Worker service timeout")
    except Exception as e:
        metrics_store["errors"] += 1
        metrics_store["requests"] += 1
        raise HTTPException(status_code=500, detail=f"å¤„ç†å¤±è´¥ / Processing failed: {str(e)}")


@app.get("/api/metrics", response_model=MetricsResponse)
async def get_metrics():
    """
    è·å–å½“å‰æ€§èƒ½æŒ‡æ ‡
    Get current performance metrics
    """
    latencies = metrics_store["latencies"]
    
    if not latencies:
        return MetricsResponse(
            timestamp=datetime.now(),
            p50_latency_ms=0.0,
            p95_latency_ms=0.0,
            p99_latency_ms=0.0,
            error_rate=0.0,
            request_count=0,
            deployment_phase=settings.deployment_phase,
            regression_type=None
        )
    
    # è®¡ç®—ç™¾åˆ†ä½æ•° / Calculate percentiles
    sorted_latencies = sorted(latencies)
    n = len(sorted_latencies)
    
    p50 = sorted_latencies[int(n * 0.50)] if n > 0 else 0.0
    p95 = sorted_latencies[int(n * 0.95)] if n > 0 else 0.0
    p99 = sorted_latencies[int(n * 0.99)] if n > 0 else 0.0
    
    # è®¡ç®—é”™è¯¯ç‡ / Calculate error rate
    total_requests = metrics_store["requests"]
    error_rate = (metrics_store["errors"] / total_requests * 100) if total_requests > 0 else 0.0
    
    # ç¡®å®šå›å½’ç±»å‹ / Determine regression type
    regression_type = None
    if regression_injector.cpu_enabled:
        regression_type = "CPU"
    elif regression_injector.db_enabled:
        regression_type = "DB"
    elif regression_injector.dependency_enabled:
        regression_type = "Dependency"
    
    return MetricsResponse(
        timestamp=datetime.now(),
        p50_latency_ms=round(p50, 2),
        p95_latency_ms=round(p95, 2),
        p99_latency_ms=round(p99, 2),
        error_rate=round(error_rate, 2),
        request_count=total_requests,
        deployment_phase=settings.deployment_phase,
        regression_type=regression_type
    )


@app.post("/api/regression/{regression_type}")
async def inject_regression(regression_type: str, request: RegressionRequest):
    """
    æ³¨å…¥å›å½’ç«¯ç‚¹
    Inject regression endpoint
    
    æ”¯æŒçš„å›å½’ç±»å‹ / Supported regression types:
    - cpu: CPUå›å½’
    - db: æ•°æ®åº“å›å½’
    - dependency: ä¸‹æ¸¸ä¾èµ–å›å½’
    """
    if regression_type == "cpu":
        regression_injector.set_cpu_regression(request.enabled)
        if request.intensity:
            settings.cpu_regression_intensity = request.intensity
    elif regression_type == "db":
        regression_injector.set_db_regression(request.enabled)
        if request.intensity:
            settings.db_regression_intensity = request.intensity
    elif regression_type == "dependency":
        regression_injector.set_dependency_regression(request.enabled)
        if request.delay_ms:
            settings.dependency_regression_delay_ms = request.delay_ms
    else:
        raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„å›å½’ç±»å‹ / Unsupported regression type: {regression_type}")
    
    return {
        "status": "success",
        "regression_type": regression_type,
        "enabled": request.enabled,
        "message": f"å›å½’æ³¨å…¥å·²{'å¯ç”¨' if request.enabled else 'ç¦ç”¨'} / Regression injection {'enabled' if request.enabled else 'disabled'}"
    }


@app.post("/api/metrics/reset")
async def reset_metrics():
    """é‡ç½®æŒ‡æ ‡ / Reset metrics"""
    global metrics_store
    metrics_store = {
        "latencies": [],
        "errors": 0,
        "requests": 0,
        "start_time": time.time()
    }
    return {"status": "success", "message": "æŒ‡æ ‡å·²é‡ç½® / Metrics reset"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
